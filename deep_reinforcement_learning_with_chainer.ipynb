{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     119,
     137,
     151,
     209,
     210,
     214,
     216
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0,-1, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1, 0,-1,-1,-1,-1,-1,-9,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0, 0, 1,-1],\n",
    "        [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]]\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1,-1,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0 ,0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1, 0,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0,-1, 0, 0, 0, 1,-1],\n",
    "        [ 0, 0, 0,-1,-1, 0, 0, 0,-1,-1,-1,-1]]\n",
    "START  = (1, 1)\n",
    "ACTION = [(-1, 0), (1, 0), (0, -1), (0, 1)] # [上, 下, 左, 右]\n",
    "EPOCH  = 10\n",
    "RESULT = []\n",
    "NUM_IN   = (len(MAZE)+1) * (len(MAZE[0])+1)\n",
    "NUM_HID1 = 450\n",
    "NUM_HID2 = 250\n",
    "NUM_HID3 = 100\n",
    "NUM_OUT  = 4\n",
    "BATCH_SIZE = 100\n",
    "ALPH   = 0.1\n",
    "EPSIL  = 0.1\n",
    "GAMMA  = np.array([0.99 for i in range(BATCH_SIZE)], np.float32)\n",
    "\n",
    "\n",
    "def q_learn(q_network, target_network, records):\n",
    "    f = open(\"/Users/chan-p/Desktop/action.txt\", \"w\")\n",
    "    result_list = []\n",
    "    state_index = init_index()\n",
    "    state_vecs, actions, rewords ,next_state_vecs, terminals = translate(records)\n",
    "    next_records = []\n",
    "    best_records = []\n",
    "    for epoch in range(EPOCH):\n",
    "        state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "        now_state = START\n",
    "        state_vec[state_index[START]] = 1\n",
    "        f.write(str(0) + \",\" + str(0) + \",\" +str(3) + \"\\n\")\n",
    "        while True:\n",
    "            # 状態sをDNN用にベクトル化\n",
    "            state_vec = np.array([state_vec], dtype = np.float32)\n",
    "            # 状態sをDNNの入力として状態sにおける各行動aの行動価値を算出：Q(s,a)\n",
    "            # 方策：e-greedy\n",
    "            action, q_max, action_list = policy_egreedy(state_vec, q_network)\n",
    "            # 次の状態s'を決定\n",
    "            next_state = (now_state[0]+action[0], now_state[1]+action[1])\n",
    "            # 即時報酬\n",
    "            # 次の状態が迷路外ならエピソード終了\n",
    "            if state_check(next_state) == 0:\n",
    "                f.write(str(action[0]) + \",\" + str(action[1]) + \",\" +str(1) + \"\\n\")\n",
    "                next_records.append((now_state, action, -1, 1))\n",
    "                result_list.append(0)\n",
    "                break\n",
    "            reword = MAZE[next_state[0]][next_state[1]]\n",
    "            if reword == 1:\n",
    "                print(epoch)\n",
    "                f.write(str(0) + \",\" + str(0) + \",\" +str(2) + \"\\n\")\n",
    "                best_records.append((now_state, action, 1, 1))\n",
    "                result_list.append(1)\n",
    "                break\n",
    "            next_records.append((now_state, action, reword/2, 0))\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vec[state_index[next_state]] = 1\n",
    "            f.write(str(action[0]) + \",\" + str(action[1]) + \",\" +str(0) + \"\\n\")\n",
    "            next_actions = target_network.forward(0, np.array([next_state_vec], dtype=np.float32))\n",
    "            y_target = reword_vectrize(next_state) + np.array([0.99 for i in range(4)]) * np.array(next_actions.data[0])\n",
    "            y_target = np.array([ALPH for i in range(4)]) * (y_target - target_network.forward(0, x_batch_next_state))\n",
    "            now_state = next_state\n",
    "                                    \n",
    "            # 学習\n",
    "            # experiment_replayによるバッチ学習\n",
    "            perm = np.random.permutation(len(records))[:BATCH_SIZE]\n",
    "            x_batch_state   = state_vecs[perm[0:BATCH_SIZE]]\n",
    "            x_batch_action  = actions[perm[0:BATCH_SIZE]]\n",
    "            x_batch_rewords = rewords[perm[0:BATCH_SIZE]]\n",
    "            x_batch_next_state = next_state_vecs[perm[0:BATCH_SIZE]]\n",
    "            # ニューラルの重み更新\n",
    "            q_network.init_grads()\n",
    "            next_actions = target_network.forward(0, x_batch_next_state)\n",
    "            max_q = np.max(next_actions.data, axis = 1)\n",
    "            y_batch_targets = x_batch_rewords + GAMMA * max_q\n",
    "            loss = q_network.forward(1, x_batch_state, y_batch_targets)\n",
    "            q_network.backpropagation(loss)\n",
    "        # レコードの更新\n",
    "        if len(next_records) + len(best_records) > 1000:\n",
    "            if len(best_records) > 500:\n",
    "                best_records = []\n",
    "            next_records.extend(best_records)\n",
    "            state_vecs, actions, rewords ,next_state_vecs, terminals = translate(next_records)\n",
    "            records = next_records\n",
    "            next_records = []\n",
    "        # target networkの更新\n",
    "        if epoch % 1 == 0:\n",
    "            target_network = update_target_network(q_network, target_network)\n",
    "    # output_qtable(target_network, state_index)\n",
    "    return result_list\n",
    "\n",
    "def update_target_network(q_network, target_network):\n",
    "    # target_network = copy.deepcopy(q_network)\n",
    "    return q_network\n",
    "        \n",
    "    \n",
    "def policy_egreedy(state, neural):\n",
    "    qvalue = neural.forward(0, state).data[0]\n",
    "    return (ACTION[random.choice([i for i, x in enumerate(qvalue) if x == max(qvalue)])] if EPSIL < random.random() else random.choice(ACTION)), max(qvalue), qvalue           \n",
    "    \n",
    "\n",
    "def reword_vectrize(next_state):\n",
    "    next_reword = []\n",
    "    for action in ACTION:\n",
    "        if  (next_state[0] + action[0]) < 0 or (next_state[0] + action[0]) > 10 or (next_state[1] + action[1]) > 9 or (next_state[1] + action[1]) < 0:\n",
    "            next_reword.append(-1)\n",
    "        else:\n",
    "            next_reword.append(MAZE[(next_state[0] + action[0])][(next_state[1] + action[1])])\n",
    "    # print(\"reword：\"+str(next_reword))\n",
    "    return np.array(next_reword, dtype = np.float32)\n",
    "    \n",
    "    \n",
    "def state_check(state):\n",
    "    if (state[0] < 0) or (state[1] < 0) or (len(MAZE)-1) < state[0] or (len(MAZE[0])-1 < state[1]) :\n",
    "        RESULT.append(0)\n",
    "        return 0\n",
    "    return 1                          \n",
    "\n",
    "def init_qtable():\n",
    "    qtable_index = {}\n",
    "    num = 0\n",
    "    for y in range(len(MAZE)+1):\n",
    "        for x in range(len(MAZE[0])+1):\n",
    "            qtable_index[(y, x)] = num\n",
    "            num += 1\n",
    "    return qtable_index\n",
    "\n",
    "def init_index():\n",
    "    qtable_index = {}\n",
    "    num = 0\n",
    "    for y in range(len(MAZE)+1):\n",
    "        for x in range(len(MAZE[0])+1):\n",
    "            qtable_index[(y, x)] = num\n",
    "            num += 1\n",
    "    return qtable_index\n",
    "\n",
    "def experience_replay():\n",
    "    records = []\n",
    "    with open(\"./record.csv\") as f:\n",
    "        for line in f:\n",
    "            line   = line[:-1].split(\",\")\n",
    "            state  = (int(line[0]), int(line[1]))\n",
    "            action = (int(line[2]), int(line[3]))\n",
    "            reword = int(line[4])\n",
    "            terminal = int(line[5])\n",
    "            record = (state, action, reword, terminal)\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def translate(records):\n",
    "    state_vecs  = []\n",
    "    actions = []\n",
    "    rewords = []\n",
    "    terminals = []\n",
    "    next_state_vecs = []\n",
    "    state_index = init_index()\n",
    "    state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "    next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "    for record in records:\n",
    "        next_state = (record[0][0]+record[1][0], record[0][1]+record[1][1])\n",
    "        state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "        state_vec[state_index[record[0]]] = 1\n",
    "        state_vecs.append(state_vec)\n",
    "        actions.append(record[1])\n",
    "        rewords.append(record[2])\n",
    "        terminals.append(record[3])\n",
    "        if record[3] == 1:\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "        else:\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vec[state_index[next_state]] = 1\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "    return np.array(state_vecs, dtype=np.float32), np.array(actions), np.array(rewords, dtype=np.float32), np.array(next_state_vecs, dtype=np.float32), np.array(terminals, dtype=np.float32)\n",
    "    \n",
    "\n",
    "def state_check(state):\n",
    "    if state[0] < 0 or state[1] < 0 or 10 < state[0] or 9 < state[1] :\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def output_qtable(q_network, state_index):\n",
    "    state_index = {v:k for k, v in state_index.items()}\n",
    "    for index in range(NUM_IN):\n",
    "        state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "        state_vec[index] = 1\n",
    "        state_vec = np.array([state_vec], dtype = np.float32)\n",
    "        q_value = q_network.forward(0, state_vec)\n",
    "        print(str(state_index[index]) + \"：\" + str(q_value.data[0]))\n",
    "\n",
    "def main():\n",
    "    q_network = neuralnetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = update_target_network(q_network, neuralnetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT))\n",
    "    return q_learn(q_network, target_network, experience_replay())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     4
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gragh(result, epoch):    \n",
    "    x = np.arange(0, epoch, 1)\n",
    "    left = np.array(x)\n",
    "    count = 0\n",
    "    count_1 = 0\n",
    "    parcent = []\n",
    "    for i in result:\n",
    "        count += 1\n",
    "        if i == 1:\n",
    "            count_1 += 1\n",
    "        parcent.append(count_1/count)\n",
    "    print(parcent[-1])\n",
    "    height = np.array(parcent)\n",
    "    plt.plot(left, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/DeXFeLuVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d0263c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_list = main()\n",
    "gragh(_list, len(_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import Function, Variable, optimizers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "class neuralnetwork:\n",
    "    def __init__(self, num_in, num_hid1, num_hid2, num_hid3, num_out):\n",
    "        self.model = Chain(hid_layer1 = L.Linear(num_in, num_hid1),\n",
    "                           hid_layer2 = L.Linear(num_hid1, num_hid2),\n",
    "                           hid_layer3 = L.Linear(num_hid2, num_hid3),\n",
    "                           out_layer  = L.Linear(num_hid3, num_out))\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "    \n",
    "    def forward(self, flg, x, t = None):\n",
    "        _x = Variable(x)\n",
    "        if flg == 1: _t = Variable(t)\n",
    "        h1  = F.dropout(F.relu(self.model.hid_layer1(_x)))\n",
    "        h2  = F.dropout(F.relu(self.model.hid_layer2(h1)))\n",
    "        h3  = F.dropout(F.relu(self.model.hid_layer3(h2)))\n",
    "        u3  = self.model.out_layer(h3)\n",
    "        # return F.softmax_cross_entropy(u2, _t) if flg else F.softmax(u2)\n",
    "        return F.mean_squared_error(self.policy_greedy(u3), _t) if flg else u3\n",
    "    \n",
    "    def backpropagation(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "    \n",
    "    def init_grads(self):\n",
    "        self.optimizer.zero_grads()\n",
    "        \n",
    "    def policy_greedy(self, actions):\n",
    "        return np.max(actions.data, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chainer.link.Chain object at 0x10695e6d8>\n",
      "[[ 0.2  0.5  0.1  0.2  0.1]\n",
      " [ 0.2  0.5  0.1  0.2  0.1]]\n",
      "as:[[ 0.09639793  0.00071677]\n",
      " [ 0.09639793  0.00071677]]\n",
      "[[ 0.09639793  0.00071677]\n",
      " [ 0.09639793  0.00071677]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.10044377  0.0038343 ]\n",
      " [ 0.10044377  0.0038343 ]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.10449059  0.00695183]\n",
      " [ 0.10449059  0.00695183]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.10853845  0.01006872]\n",
      " [ 0.10853845  0.01006872]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.11258755  0.01318443]\n",
      " [ 0.11258755  0.01318443]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.11663818  0.01629826]\n",
      " [ 0.11663818  0.01629826]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.12069059  0.01940953]\n",
      " [ 0.12069059  0.01940953]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.1247453   0.02251755]\n",
      " [ 0.1247453   0.02251755]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.12880264  0.02562148]\n",
      " [ 0.12880264  0.02562148]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "[[ 0.13286331  0.02872054]\n",
      " [ 0.13286331  0.02872054]]\n",
      "[[ 2.          1.10000002]\n",
      " [ 2.          1.10000002]]\n",
      "unknown[[-0.00991341 -0.03327346]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import random\n",
    "import traceback\n",
    "import sys\n",
    "import math\n",
    "\n",
    "#5次元のベクトルを入力して、そのベクトルがYes(1)かNo(0)かの2値分類\n",
    "#3層パーセプトロン\n",
    "model =  Chain(Hidden_Layer=L.Linear(5,13),Output_Layer=L.Linear(13,2))\n",
    "#最適化アルゴリズム：SGD\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "print(model)\n",
    "p = np.array([0.2,0.5,0.1,0.2,0.1],dtype=np.float32)\n",
    "Input = np.array([p, p],dtype=np.float32)\n",
    "print(Input)\n",
    "X = Variable(Input)\n",
    "u1 = model.Hidden_Layer(X)\n",
    "h = F.relu(u1)\n",
    "u2 = model.Output_Layer(h)\n",
    "y = u2\n",
    "print(\"as:\"+str(y.data))\n",
    "Answer1 = np.array([2.0, 1.1],dtype=np.float32)\n",
    "asd = []\n",
    "asd.append(Answer1)\n",
    "asd.append(Answer1)\n",
    "Answer = np.array(asd, dtype=np.float32)\n",
    "T = Variable(Answer)\n",
    "\n",
    "for roop in range(10):\n",
    "    optimizer.zero_grads()\n",
    "\n",
    "    u1 = model.Hidden_Layer(X)\n",
    "    h = F.relu(u1)\n",
    "    u2 = model.Output_Layer(h)\n",
    "    y = u2\n",
    "    print(y.data)\n",
    "    print(T.data)\n",
    "    # loss = F.softmax_cross_entropy(u2,T)\n",
    "    loss = F.mean_squared_error(y,T)\n",
    "    loss.backward()\n",
    "    optimizer.weight_decay(0.8)\n",
    "    optimizer.update()\n",
    "\n",
    "\n",
    "p = np.array([0.22,0.13,0.99,0.3,0.2],dtype=np.float32)\n",
    "Input = np.array([p],dtype=np.float32)\n",
    "X = Variable(Input)\n",
    "\n",
    "\n",
    "u1 = model.Hidden_Layer(X)\n",
    "h = F.relu(u1)\n",
    "u2 = model.Output_Layer(h)\n",
    "y = u2\n",
    "print(\"unknown\" + str(y.data))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
