{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": [
     101,
     105,
     133,
     136,
     139,
     142
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "NUM_IN   = 2\n",
    "NUM_HID1 = 1000\n",
    "NUM_HID2 = 500\n",
    "NUM_HID3 = 250\n",
    "NUM_OUT  = 3\n",
    "BATCH_SIZE = 32\n",
    "ALPH   = 0.1\n",
    "ACTION = [-2, 0, 2]\n",
    "EPOCH = 100\n",
    "GAMMA = 0.1\n",
    "EPSIL = 0.3\n",
    "DQNMODEL_PATH = \"./DQN_model/mounaincar.model\"\n",
    "\n",
    "def deep_q_learn(env, q_network, target_network, experince):\n",
    "    # プロッセサーの選択\n",
    "    xp = switch_ProccerUnit(\"CPU\")\n",
    "    epsil = EPSIL\n",
    "    record = []\n",
    "    for episode in range(1, EPOCH+1):\n",
    "        print(episode)\n",
    "        # 初期状態の定義\n",
    "        now_state = env.reset()\n",
    "        pos, spe = get_state(now_state)\n",
    "        state_vec = get_state_vec(pos, spe, xp, 2)\n",
    "        epsil = reduce_epsil(episode, epsil)\n",
    "        for step in range(2000):\n",
    "            # 方策に従って行動を選択\n",
    "            action, _ = policy_egreedy_tri(state_vec, q_network, epsil, xp)\n",
    "            # 行動による次状態を観測\n",
    "            next_state, reward, terminal, info = agent_action(env, action)\n",
    "            next_pos, next_spe = get_state(next_state)\n",
    "            deep_learn(action, q_network, state_vec, xp, [make_target(action, reward, next_state, terminal, target_network, xp)])\n",
    "            if reward == 1:\n",
    "                stock_record(pos, spe, action, 0, 0, reward, 1)\n",
    "                break\n",
    "            # 状態更新\n",
    "            next_pos, next_spe = get_state(next_state)\n",
    "            state_vec = get_state_vec(next_pos, next_spe, xp, 2)\n",
    "            # レコード蓄積\n",
    "            record = stock_record(record, pos, spe, action, next_pos, next_state, reward, 0)\n",
    "            pos, spe = next_pos, next_spe\n",
    "            # env.render()\n",
    "            # Experience_Replayによるバッチ学習\n",
    "            if len(record) > 500:\n",
    "                experience_replay(record, q_network, target_network, xp)\n",
    "                if len(record) > 2000:\n",
    "                    record = []\n",
    "            # Target_networkの更新\n",
    "            if step % 200 == 0:\n",
    "                q_network.save_weight(DQNMODEL_PATH)\n",
    "        \n",
    "def get_state(state):\n",
    "    return state[0], state[1]\n",
    "\n",
    "def get_state_vec(pos, spe, xp, flg):\n",
    "    vec = xp.array([pos, spe], dtype=xp.float32)\n",
    "    if flg == 1: return vec\n",
    "    return xp.array([vec], dtype=xp.float32) \n",
    "        \n",
    "    \n",
    "    \n",
    "def experience_replay(record, q_network, target_network, xp):\n",
    "    state_vecs, actions, next_states, rewards, terminals = transelate(record, xp)\n",
    "    perm = xp.random.permutation(len(record))[:BATCH_SIZE]\n",
    "    x_batch_state_vecs = state_vecs[perm[0:BATCH_SIZE]]\n",
    "    x_batch_action     = actions[perm[0:BATCH_SIZE]]\n",
    "    x_batch_rewards    = rewards[perm[0:BATCH_SIZE]]\n",
    "    x_batch_terminals  = terminals[perm[0:BATCH_SIZE]]\n",
    "    y_batch_targets    = []\n",
    "    for index in range(BATCH_SIZE):\n",
    "        y_batch_targets.append(make_target(x_batch_action[index], x_batch_rewards[index], next_states[perm[index]], x_batch_tereminals[index], xp))\n",
    "    deep_learn(x_batch_action, q_network, x_batch_state_vecs, xp, y_targets)\n",
    "\n",
    "def transelate(record, xp):\n",
    "    state_vecs  = []\n",
    "    actions     = []\n",
    "    rewards     = []\n",
    "    terminals   = []\n",
    "    next_states = []\n",
    "    for data in record:\n",
    "        state_vecs.append(get_state_vec(data[0][0],data[0][1], xp, 1))\n",
    "        actions.append(data[1])\n",
    "        next_states.append(data[2])\n",
    "        rewards.append(data[3])\n",
    "        terminals.append(data[4])\n",
    "    return xp.array(state_vecs, dtype=xp.float32), xp.array(actions), next_states, xp.array(rewards), xp.array(terminals)   \n",
    "          \n",
    "def stock_record(record, pos, spe, action, next_pos, next_spe, reward, terminal):\n",
    "    record.append([(pos, spe), action, (next_pos, next_pos), reward, terminal])\n",
    "    return record\n",
    "    \n",
    "def make_target(action, reward, next_state, tereminal, target_network, xp):\n",
    "    y_target = [0 for i in range(3)]\n",
    "    _, max_q = policy_egreedy_tri(get_state_vec(next_state[0], next_state[1], xp, 2), target_network, 0, xp)\n",
    "    y_target[action] = reward if tereminal else reward + GAMMA * max_q    \n",
    "    y_target = xp.array(y_target, dtype=np.float32)\n",
    "    return y_target\n",
    "\n",
    "def deep_learn(action, q_network, state_vec, xp, y_target, flg=None):\n",
    "    y_target = xp.array(y_target, dtype=xp.float32)\n",
    "    q_network.init_grads()\n",
    "    loss = q_network.forward(1, state_vec, y_target)\n",
    "    q_network.backpropagation(loss)\n",
    "    \n",
    "def policy_egreedy_tri(state, neural, epsil, xp):\n",
    "    import scipy.spatial.distance\n",
    "    qvalue_list = []\n",
    "    tmp = []\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_vec = np.array(neural.forward(0, state).data[0])\n",
    "    for qvalue in qvalue_list:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(xp.array(qvalue), qvalue_vec)\n",
    "        tmp.append(sim)\n",
    "    if tmp[0] < tmp[1]:\n",
    "        return (list(qvalue_list[1]).index(max(qvalue_list[1])) if random.random()>epsil else random.choice([0,1,2])), max(qvalue_list[1]) \n",
    "    else:\n",
    "        return (list(qvalue_list[0]).index(max(qvalue_list[0])) if random.random()>epsil else random.choice([0,1,2])), max(qvalue_list[0])\n",
    "        \n",
    "def agent_action(env, action):\n",
    "    return env.step(action)\n",
    "\n",
    "def update_target_network(q_network):\n",
    "    return copy.deepcopy(q_network)\n",
    "\n",
    "def reduce_epsil(epoch, epsil):\n",
    "    return epsil\n",
    "                 \n",
    "def switch_ProccerUnit(pu):\n",
    "    return cuda.cupy if pu == \"GPU\" else np\n",
    "                 \n",
    "def main():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    q_network.load_weight(DQNMODEL_PATH)\n",
    "    target_network = update_target_network(q_network)\n",
    "    deep_q_learn(env, q_network, target_network, [0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hid1, num_hid2, num_hid3, num_out):\n",
    "        self.model = Chain(hid_layer1 = L.Linear(num_in, num_hid1),\n",
    "                           hid_layer2 = L.Linear(num_hid1, num_hid2),\n",
    "                           hid_layer3 = L.Linear(num_hid2, num_hid3),\n",
    "                           out_layer  = L.Linear(num_hid3, num_out))\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "    \n",
    "    def forward(self, flg, x, t = None):\n",
    "        _x = Variable(x)\n",
    "        if flg == 1: _t = Variable(t)\n",
    "        h1  = F.dropout(F.relu(self.model.hid_layer1(_x)))\n",
    "        h2  = F.dropout(F.relu(self.model.hid_layer2(h1)))\n",
    "        h3  = F.dropout(F.relu(self.model.hid_layer3(h2)))\n",
    "        u3  = self.model.out_layer(h3)\n",
    "        # return F.softmax_cross_entropy(u2, _t) if flg else F.softmax(u2)\n",
    "        # return F.mean_squared_error(self.policy_greedy(u3), _t) if flg else u3\n",
    "        return F.mean_squared_error(u3, _t) if flg else u3\n",
    "    \n",
    "    def backpropagation(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "    \n",
    "    def init_grads(self):\n",
    "        self.optimizer.zero_grads()\n",
    "        \n",
    "    def save_weight(self, model):\n",
    "        serializers.save_npz(model, self.model)\n",
    "        \n",
    "    def load_weight(self, model):\n",
    "        serializers.load_npz(model, self.model)\n",
    "        \n",
    "    def policy_greedy(self, actions):\n",
    "        return np.max(actions.data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-12 20:51:20,338] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_batch_tereminal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-636a9fd80deb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQNMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mtarget_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mdeep_q_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-636a9fd80deb>\u001b[0m in \u001b[0;36mdeep_q_learn\u001b[0;34m(env, q_network, target_network, experince)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Experience_Replayによるバッチ学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-636a9fd80deb>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(record, q_network, target_network, xp)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0my_batch_targets\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0my_batch_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_tereminal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mdeep_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_state_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_batch_tereminal' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0,-1, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1, 0,-1,-1,-1,-1,-1,-9,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0, 0, 1,-1],\n",
    "        [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]]\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1,-1,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0 ,0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1, 0,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0,-1, 0, 0, 0, 1,-1],\n",
    "        [ 0, 0, 0,-1,-1, 0, 0, 0,-1,-1,-1,-1]]\n",
    "START  = (1, 1)\n",
    "ACTION = [(-1, 0), (1, 0), (0, -1), (0, 1)] # [上, 下, 左, 右]\n",
    "EPOCH  = 30\n",
    "RESULT = []\n",
    "NUM_IN   = (len(MAZE)+1) * (len(MAZE[0])+1)\n",
    "NUM_HID1 = 1000\n",
    "NUM_HID2 = 500\n",
    "NUM_HID3 = 250\n",
    "NUM_OUT  = 4\n",
    "BATCH_SIZE = 32\n",
    "ALPH   = 0.1\n",
    "GAMMA  = np.array([0.99 for i in range(BATCH_SIZE)], np.float32)\n",
    "\n",
    "def deep_qq_learn(q_network, target_network, records):\n",
    "    file = open(\"/Users/chan-p/Desktop/action2.txt\", \"w\")\n",
    "    result_list  = []\n",
    "    next_records = []\n",
    "    best_records = []\n",
    "    state_index  = init_index()\n",
    "    EPSIL  = 0.1\n",
    "    state_vecs, actions, rewords ,next_state_vecs, terminals, next_states, now_states= translate(records)\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        EPSIL = decay_EPSIL(epoch, EPSIL)\n",
    "        state_vec = init_state_vec()\n",
    "        now_state = START\n",
    "        state_vec[state_index[START]] = 1\n",
    "        record_write(file, 0, 0, 3)\n",
    "        episode_records = []\n",
    "        while True:\n",
    "            # 状態sをDNN用にベクトル化\n",
    "            state_vec = get_state_vec(state_vec)\n",
    "            # 状態sをDNNの入力として状態sにおける各行動aの行動価値を算出：Q(s,a)\n",
    "            # 方策：e-greedy\n",
    "            if epoch < EPOCH*(1/100):\n",
    "                action, q_max, action_list = policy_egreedy(state_vec, q_network, EPSIL)\n",
    "            else:\n",
    "                action, q_max, action_list = policy_greedy_tri(state_vec, q_network, EPSIL)\n",
    "            # 次の状態s'を決定\n",
    "            next_state = get_next_state(now_state, action)\n",
    "            # 次の状態が迷路外ならエピソード終了\n",
    "            if state_check(next_state) == 0 or MAZE[next_state[0]][next_state[1]] == -1:\n",
    "                print(now_state)\n",
    "                # 罰則による学習\n",
    "                deep_learn(q_network, target_network, state_vec, now_state, next_state, 0)\n",
    "                record_write(file, action[0], action[1], 1)\n",
    "                next_records.append((now_state, action, -1, 1))\n",
    "                next_records.extend(episode_records)\n",
    "                result_list.append(0)\n",
    "                break\n",
    "            # 即時報酬\n",
    "            reward = MAZE[next_state[0]][next_state[1]]\n",
    "            if reward == 1:\n",
    "                print(epoch)\n",
    "                # 成功報酬による学習\n",
    "                deep_learn(q_network, target_network, state_vec, now_state, next_state, 0)\n",
    "                record_write(file, 0, 0, 2)\n",
    "                # 成功体験を優先してExperiment_Replayレコードに追加\n",
    "                best_records.append((now_state, action, 1, 1))\n",
    "                best_records.extend(episode_records)\n",
    "                result_list.append(1)\n",
    "                break\n",
    "            # 実行動による学習\n",
    "            deep_learn(q_network, target_network, state_vec, now_state, next_state, 1)\n",
    "            # Experiment_Replayレコードに追加\n",
    "            episode_records.append((now_state, action, reward, 0))\n",
    "            record_write(file, action[0], action[1], 0)\n",
    "            now_state = next_state\n",
    "            state_vec = init_state_vec()\n",
    "            state_vec[state_index[now_state]] = 1\n",
    "\n",
    "            # 学習\n",
    "            # experiment_replayによるバッチ学習\n",
    "            perm = np.random.permutation(len(records))[:BATCH_SIZE]\n",
    "            x_batch_state   = state_vecs[perm[0:BATCH_SIZE]]\n",
    "            x_batch_action  = actions[perm[0:BATCH_SIZE]]\n",
    "            x_batch_rewords = rewords[perm[0:BATCH_SIZE]]\n",
    "            x_batch_now_state = []\n",
    "            x_batch_next_state = []\n",
    "            for index in perm:\n",
    "                x_batch_now_state.append(now_states[index])\n",
    "                x_batch_next_state.append(next_states[index])\n",
    "            x_batch_next_state_vec = next_state_vecs[perm[0:BATCH_SIZE]]\n",
    "            y_batch_target = []\n",
    "            # ニューラルの重み更新\n",
    "            for index in range(BATCH_SIZE):\n",
    "                y_batch_target.append(deep_learn(q_network, target_network, x_batch_state[index], x_batch_now_state[index], x_batch_next_state[index], 1))\n",
    "            y_batch_target = np.array(y_batch_target, dtype=np.float32)\n",
    "            q_network.init_grads()\n",
    "            loss = q_network.forward(1, x_batch_state, y_batch_target)\n",
    "            q_network.backpropagation(loss)\n",
    "              \n",
    "        # レコードの更新\n",
    "        if len(next_records) + len(best_records) > 5000:\n",
    "            best_records.extend(next_records)\n",
    "            state_vecs, actions, rewords ,next_state_vecs, terminals, next_states, now_states = translate(best_records)\n",
    "            records = copy.copy(best_records)\n",
    "            if len(best_records) > 5000:\n",
    "                best_records = []\n",
    "            next_records = []\n",
    "        \n",
    "        # target networkの更新\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch)\n",
    "            target_network = update_target_network(q_network)\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            q_network.save_weight()\n",
    "            \n",
    "    file.close()\n",
    "    return result_list\n",
    "\n",
    "def init_state_vec():\n",
    "    return np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "\n",
    "def get_state_vec(state_vec):\n",
    "    return np.array([state_vec], dtype = np.float32)\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    return (state[0]+action[0], state[1]+action[1])\n",
    "\n",
    "def deep_learn(q_network, target_network, state_vec, now_state, next_state, flg):\n",
    "    y_targets = []\n",
    "    state_index = init_index()\n",
    "    for action in ACTION:\n",
    "        next_state_vec = init_state_vec()\n",
    "        next_state = get_next_state(now_state, action)\n",
    "        # 次の状態が迷路外\n",
    "        # 報酬(罰則)のみ\n",
    "        if next_state[0] < 0 or next_state[0] > len(MAZE)-1 or next_state[1] > len(MAZE[0])-1 or next_state[1] < 0 or MAZE[next_state[0]][next_state[1]] == -1:\n",
    "            y_target = -2\n",
    "        else:\n",
    "            next_state_vec[state_index[(next_state[0], next_state[1])]] = 1\n",
    "            next_actions = target_network.forward(0, np.array([next_state_vec], dtype=np.float32))\n",
    "            max_q = np.max(next_actions.data[0], axis = 0)\n",
    "            y_target = MAZE[next_state[0]][next_state[1]] + 0.1 * max_q\n",
    "        y_targets.append(y_target)\n",
    "    y_targets = np.array(y_targets, dtype=np.float32)\n",
    "    if flg: return y_targets\n",
    "    y_target = np.array([y_targets], dtype=np.float32)\n",
    "    q_network.init_grads()\n",
    "    loss = q_network.forward(1, state_vec, y_target)\n",
    "    q_network.backpropagation(loss)\n",
    "\n",
    "def update_target_network(q_network):\n",
    "    return copy.deepcopy(q_network)\n",
    "\n",
    "def state_check(state):\n",
    "    if (state[0] < 0) or (state[1] < 0) or (len(MAZE)-1) < state[0] or (len(MAZE[0])-1 < state[1]):\n",
    "        RESULT.append(0)\n",
    "        return 0\n",
    "    return 1 \n",
    "\n",
    "def policy_egreedy(state, neural, EPSIL):\n",
    "    qvalue = neural.forward(0, state).data[0]\n",
    "    return (ACTION[random.choice([i for i, x in enumerate(qvalue) if x == max(qvalue)])] if EPSIL < random.random() else random.choice(ACTION)), max(qvalue), qvalue           \n",
    "\n",
    "def policy_greedy_tri(state, neural, EPSIL):\n",
    "    import scipy.spatial.distance\n",
    "    qvalue_list = []\n",
    "    tmp = []\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_vec = np.array(neural.forward(0, state).data[0])\n",
    "    for qvalue in qvalue_list:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(np.array(qvalue), qvalue_vec)\n",
    "        tmp.append(sim)\n",
    "    if tmp[0] < tmp[1]:\n",
    "        return ACTION[list(qvalue_list[1]).index(max(qvalue_list[1]))], max(qvalue_list[1]), qvalue_list[1]\n",
    "    else:\n",
    "        return ACTION[list(qvalue_list[0]).index(max(qvalue_list[0]))], max(qvalue_list[0]), qvalue_list[0]\n",
    "\n",
    "def translate(records):\n",
    "    now_states = []\n",
    "    state_vecs  = []\n",
    "    actions = []\n",
    "    rewords = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    next_state_vecs = []\n",
    "    state_index = init_index()\n",
    "    state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "    next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "    for record in records:\n",
    "        now_states.append(record[0])\n",
    "        next_state = (record[0][0]+record[1][0], record[0][1]+record[1][1])\n",
    "        next_states.append(next_state)\n",
    "        state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "        state_vec[state_index[record[0]]] = 1\n",
    "        state_vecs.append(state_vec)\n",
    "        actions.append(record[1])\n",
    "        rewords.append(record[2])\n",
    "        terminals.append(record[3])\n",
    "        if record[3] == 1:\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "        else:\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vec[state_index[next_state]] = 1\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "    return np.array(state_vecs, dtype=np.float32), np.array(actions), np.array(rewords, dtype=np.float32), np.array(next_state_vecs, dtype=np.float32), np.array(terminals, dtype=np.float32), next_states, now_states\n",
    "\n",
    "def experience_replay():\n",
    "    records = []\n",
    "    with open(\"./record.csv\") as f:\n",
    "        for line in f:\n",
    "            line   = line[:-1].split(\",\")\n",
    "            state  = (int(line[0]), int(line[1]))\n",
    "            action = (int(line[2]), int(line[3]))\n",
    "            reword = int(line[4])\n",
    "            terminal = int(line[5])\n",
    "            record = (state, action, reword, terminal)\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def record_write(file, state_y, state_x, terminal):\n",
    "    file.write(str(state_y) + \",\" + str(state_x) + \",\" +str(terminal) + \"\\n\")\n",
    "\n",
    "def init_index():\n",
    "    qtable_index = {}\n",
    "    num = 0\n",
    "    for y in range(len(MAZE)+1):\n",
    "        for x in range(len(MAZE[0])+1):\n",
    "            qtable_index[(y, x)] = num\n",
    "            num += 1\n",
    "    return qtable_index\n",
    "\n",
    "def decay_EPSIL(epoch, EPSIL):\n",
    "    if epoch > (EPOCH/3)*2:\n",
    "        return EPSIL/(epoch)*(EPOCH/10)\n",
    "    return EPSIL\n",
    "\n",
    "def main():\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    q_network.load_weight()\n",
    "    target_network = update_target_network(q_network)\n",
    "    return deep_q_learn(q_network, target_network, experience_replay())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import gym\n",
    "import copy\n",
    "\n",
    "def pp(env):\n",
    "    return copy.deepcopy(env)\n",
    "\n",
    "def main1():\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    # env1 = gym.make('MountainCar-v0')\n",
    "    ob1 = env.reset()\n",
    "    # ob2 = env1.reset()\n",
    "    ob2 = ob1\n",
    "    env1 = copy.deepcopy(env)\n",
    "    for i in range(3100):\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        # action = 2\n",
    "        print(\"asd:\" + str(env.step(2)))\n",
    "        for o in range(10):\n",
    "            action = env1.action_space.sample()\n",
    "            env1.step(action)\n",
    "            env1 = pp(env)\n",
    "            if o == 9999: print(env1.step(2))\n",
    "        print(\"asd:\" + str(env.step(2)))\n",
    "        env1 = copy.deepcopy(env)\n",
    "        print(str(env.step(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
