{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     110,
     138,
     141,
     145
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "NUM_IN   = 6\n",
    "NUM_HID1 = 1000\n",
    "NUM_HID2 = 500\n",
    "NUM_HID3 = 250\n",
    "NUM_OUT  = 3\n",
    "BATCH_SIZE = 100\n",
    "ACTION = [-2, 0, 2]\n",
    "EPOCH = 20\n",
    "GAMMA = 0.3\n",
    "EPSIL = 0.1\n",
    "DQNMODEL_PATH = \"./DQN_model/acrobotver2.model\"\n",
    "\n",
    "def deep_q_learn(env, q_network, target_network, experince):\n",
    "    # プロッセサーの選択\n",
    "    xp = switch_ProccerUnit(\"CPU\")\n",
    "    epsil = EPSIL\n",
    "    record = []\n",
    "    for episode in range(1, EPOCH+1):\n",
    "        print(episode)\n",
    "        # 初期状態の定義\n",
    "        now_state = env.reset()\n",
    "        state_vec = get_state_vec(now_state, xp, 2)\n",
    "        epsil = reduce_epsil(episode, epsil)\n",
    "        for step in range(20000):\n",
    "            # 方策に従って行動を選択\n",
    "            action, _, action_list = policy_egreedy_tri(state_vec, q_network, epsil, xp)\n",
    "            # 行動による次状態を観測\n",
    "            next_state, reward, terminal, info = agent_action(env, action)\n",
    "            #if reward != -1: print(next_state, reward)\n",
    "            deep_learn(action, q_network, state_vec, xp, [make_target(action, reward, next_state, terminal, target_network, xp, action_list)])\n",
    "            if reward == 1:\n",
    "                print(\"成功\")\n",
    "                record = stock_record(record, now_state, action, 0, reward, True, action_list)\n",
    "                break\n",
    "            # 状態更新\n",
    "            state_vec = get_state_vec(next_state, xp, 2)\n",
    "            # レコード蓄積\n",
    "            record = stock_record(record, now_state, action, next_state, reward, 0, action_list)\n",
    "            now_state = next_state\n",
    "            env.render()\n",
    "            \n",
    "            # Experience_Replayによるバッチ学習\n",
    "            if len(record) > 1000:\n",
    "                # experience_replay(record, q_network, target_network, xp)\n",
    "                if len(record) > 1400:\n",
    "                    record = []\n",
    "            \n",
    "            # Target_networkの更新\n",
    "            if step % 200 == 0:\n",
    "                print(step)\n",
    "                q_network.save_weight(DQNMODEL_PATH)\n",
    "                target_network = update_target_network(q_network)\n",
    "            \n",
    "def get_state_vec(state, xp, flg):\n",
    "    vec = xp.array([state[0], state[1], state[2], state[3], state[4], state[5]], dtype=xp.float32)\n",
    "    if flg == 1: return vec\n",
    "    return xp.array([vec], dtype=xp.float32) \n",
    "        \n",
    "def experience_replay(record, q_network, target_network, xp):\n",
    "    state_vecs, actions, next_states, rewards, terminals, action_lists = transelate(record, xp)\n",
    "    perm = xp.random.permutation(len(record))[:BATCH_SIZE]\n",
    "    x_batch_state_vecs   = state_vecs[perm[0:BATCH_SIZE]]\n",
    "    x_batch_action       = actions[perm[0:BATCH_SIZE]]\n",
    "    x_batch_rewards      = rewards[perm[0:BATCH_SIZE]]\n",
    "    x_batch_terminals    = terminals[perm[0:BATCH_SIZE]]\n",
    "    x_batch_action_lists = action_lists[perm[0:BATCH_SIZE]]\n",
    "    y_batch_targets      = []\n",
    "    for index in range(BATCH_SIZE):\n",
    "        y_batch_targets.append(make_target(x_batch_action[index], x_batch_rewards[index], next_states[perm[index]], x_batch_terminals[index], target_network, xp, action_lists[index]))\n",
    "    deep_learn(x_batch_action, q_network, x_batch_state_vecs, xp, y_batch_targets)\n",
    "\n",
    "def transelate(record, xp):\n",
    "    state_vecs  = []\n",
    "    actions     = []\n",
    "    rewards     = []\n",
    "    terminals   = []\n",
    "    next_states = []\n",
    "    action_lists= []\n",
    "    for data in record:\n",
    "        state_vecs.append(get_state_vec(data[0], xp, 1))\n",
    "        actions.append(data[1])\n",
    "        next_states.append(data[2])\n",
    "        rewards.append(data[3])\n",
    "        terminals.append(data[4])\n",
    "        action_lists.append(data[5])\n",
    "    return xp.array(state_vecs, dtype=xp.float32), xp.array(actions), next_states, xp.array(rewards), xp.array(terminals), xp.array(action_lists)   \n",
    "    \n",
    "def stock_record(record, now_state, action, next_state, reward, terminal, action_list):\n",
    "    record.append([(now_state[0], now_state[1],now_state[2], now_state[3], now_state[4], now_state[5]), action, (next_state[0], next_state[1],next_state[2], next_state[3], next_state[4], next_state[5]), reward, terminal, action_list])\n",
    "    return record\n",
    "    \n",
    "def make_target(action, reward, next_state, tereminal, target_network, xp, action_list):\n",
    "    y_target = copy.deepcopy(action_list)\n",
    "    _, max_q, _ = policy_egreedy_tri(get_state_vec(next_state, xp, 2), target_network, 0, xp)\n",
    "    y_target[action] = reward if tereminal else reward + GAMMA * max_q    \n",
    "    y_target = xp.array(y_target, dtype=xp.float32)\n",
    "    return y_target\n",
    "\n",
    "def deep_learn(action, q_network, state_vec, xp, y_target, flg=None):\n",
    "    y_target = xp.array(y_target, dtype=xp.float32)\n",
    "    q_network.init_grads()\n",
    "    loss = q_network.forward(1, state_vec, y_target)\n",
    "    q_network.backpropagation(loss)\n",
    "    \n",
    "    \n",
    "def policy_egreedy_tri(state, neural, epsil, xp):\n",
    "    import scipy.spatial.distance\n",
    "    qvalue_list = []\n",
    "    tmp = []\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_vec = np.array(neural.forward(0, state).data[0])\n",
    "    for qvalue in qvalue_list:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(xp.array(qvalue), qvalue_vec)\n",
    "        tmp.append(sim)\n",
    "    if tmp[0] < tmp[1]:\n",
    "        return (list(qvalue_list[1]).index(max(qvalue_list[1])) if random.random()>epsil else random.choice([0,1,2])), max(qvalue_list[1]), qvalue_list[1]\n",
    "    else:\n",
    "        return (list(qvalue_list[0]).index(max(qvalue_list[0])) if random.random()>epsil else random.choice([0,1,2])), max(qvalue_list[0]), qvalue_list[0]\n",
    "        \n",
    "        \n",
    "def agent_action(env, action):\n",
    "    next_state, reward, terminal, info = env.step(action)\n",
    "    return next_state, reward, False, info\n",
    "\n",
    "def update_target_network(q_network):\n",
    "    return copy.deepcopy(q_network)\n",
    "\n",
    "def reduce_epsil(epoch, epsil):\n",
    "    return epsil\n",
    "                 \n",
    "    \n",
    "def switch_ProccerUnit(pu):\n",
    "    return cuda.cupy if pu == \"GPU\" else np\n",
    "                 \n",
    "    \n",
    "def main():\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    #q_network.load_weight(DQNMODEL_PATH)\n",
    "    target_network = update_target_network(q_network)\n",
    "    deep_q_learn(env, q_network, target_network, [0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hid1, num_hid2, num_hid3, num_out):\n",
    "        self.model = Chain(hid_layer1 = L.Linear(num_in, num_hid1),\n",
    "                           hid_layer2 = L.Linear(num_hid1, num_hid2),\n",
    "                           hid_layer3 = L.Linear(num_hid2, num_hid3),\n",
    "                           out_layer  = L.Linear(num_hid3, num_out))\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "    \n",
    "    def forward(self, flg, x, t = None):\n",
    "        _x = Variable(x)\n",
    "        if flg == 1: _t = Variable(t)\n",
    "        h1  = F.dropout(F.relu(self.model.hid_layer1(_x)))\n",
    "        h2  = F.dropout(F.relu(self.model.hid_layer2(h1)))\n",
    "        h3  = F.dropout(F.relu(self.model.hid_layer3(h2)))\n",
    "        u3  = self.model.out_layer(h3)\n",
    "        # return F.mean_squared_error(self.policy_greedy(u3), _t) if flg else u3\n",
    "        return F.mean_squared_error(u3, _t) if flg else u3\n",
    "    \n",
    "    def backpropagation(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "    \n",
    "    def init_grads(self):\n",
    "        self.optimizer.zero_grads()\n",
    "        \n",
    "    def save_weight(self, model):\n",
    "        serializers.save_npz(model, self.model)\n",
    "        \n",
    "    def load_weight(self, model):\n",
    "        serializers.load_npz(model, self.model)\n",
    "        \n",
    "    def policy_greedy(self, actions):\n",
    "        return np.max(actions.data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import gym\n",
    "env = gym.make('Acrobot-v1')\n",
    "print(env.reset())\n",
    "print(env.action_space)\n",
    "print(env.step(0))\n",
    "for _ in range(1000):\n",
    "    # env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    # print(env.action_space.sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
