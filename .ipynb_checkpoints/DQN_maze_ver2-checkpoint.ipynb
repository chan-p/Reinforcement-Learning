{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13,
     25,
     150,
     153,
     156,
     159,
     188,
     194,
     198,
     213,
     243,
     256,
     259,
     268,
     273
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0,-1, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1, 0,-1,-1,-1,-1,-1,-9,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0, 0, 1,-1],\n",
    "        [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]]\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1,-1,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0 ,0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1, 0,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0,-1, 0, 0, 0, 1,-1],\n",
    "        [ 0, 0, 0,-1,-1, 0, 0, 0,-1,-1,-1,-1]]\n",
    "START  = (1, 1)\n",
    "ACTION = [(-1, 0), (1, 0), (0, -1), (0, 1)] # [上, 下, 左, 右]\n",
    "EPOCH  = 5\n",
    "RESULT = []\n",
    "NUM_IN   = (len(MAZE)+1) * (len(MAZE[0])+1)\n",
    "NUM_HID1 = 1000\n",
    "NUM_HID2 = 500\n",
    "NUM_HID3 = 250\n",
    "NUM_OUT  = 4\n",
    "BATCH_SIZE = 32\n",
    "ALPH   = 0.1\n",
    "GAMMA  = np.array([0.99 for i in range(BATCH_SIZE)], np.float32)\n",
    "\n",
    "def deep_q_learn(q_network, target_network, records):\n",
    "    file = open(\"/Users/chan-p/Desktop/action2.txt\", \"w\")\n",
    "    result_list  = []\n",
    "    next_records = []\n",
    "    best_records = []\n",
    "    state_index  = init_index()\n",
    "    EPSIL  = 0.1\n",
    "    state_vecs, actions, rewords ,next_state_vecs, terminals, next_states, now_states= translate(records)\n",
    "    for epoch in range(1, EPOCH+1):\n",
    "        EPSIL = decay_EPSIL(epoch, EPSIL)\n",
    "        state_vec = init_state_vec()\n",
    "        now_state = START\n",
    "        state_vec[state_index[START]] = 1\n",
    "        record_write(file, 0, 0, 3)\n",
    "        episode_records = []\n",
    "        while True:\n",
    "            # 状態sをDNN用にベクトル化\n",
    "            state_vec = get_state_vec(state_vec)\n",
    "            # 状態sをDNNの入力として状態sにおける各行動aの行動価値を算出：Q(s,a)\n",
    "            # 方策：e-greedy\n",
    "            if epoch < EPOCH*(1/100):\n",
    "                action, q_max, action_list = policy_egreedy(state_vec, q_network, EPSIL)\n",
    "            else:\n",
    "                action, q_max, action_list = policy_greedy_tri(state_vec, q_network, EPSIL)\n",
    "            # 次の状態s'を決定\n",
    "            next_state = get_next_state(now_state, action)\n",
    "            # 次の状態が迷路外ならエピソード終了\n",
    "            if state_check(next_state) == 0 or MAZE[next_state[0]][next_state[1]] == -1:\n",
    "                print(now_state)\n",
    "                # 罰則による学習\n",
    "                deep_learn(q_network, target_network, state_vec, now_state, next_state, 0)\n",
    "                record_write(file, action[0], action[1], 1)\n",
    "                next_records.append((now_state, action, -1, 1))\n",
    "                next_records.extend(episode_records)\n",
    "                result_list.append(0)\n",
    "                break\n",
    "            # 即時報酬\n",
    "            reward = MAZE[next_state[0]][next_state[1]]\n",
    "            if reward == 1:\n",
    "                print(epoch)\n",
    "                # 成功報酬による学習\n",
    "                deep_learn(q_network, target_network, state_vec, now_state, next_state, 0)\n",
    "                record_write(file, 0, 0, 2)\n",
    "                # 成功体験を優先してExperiment_Replayレコードに追加\n",
    "                best_records.append((now_state, action, 1, 1))\n",
    "                best_records.extend(episode_records)\n",
    "                result_list.append(1)\n",
    "                break\n",
    "            # 実行動による学習\n",
    "            deep_learn(q_network, target_network, state_vec, now_state, next_state, 1)\n",
    "            # Experiment_Replayレコードに追加\n",
    "            episode_records.append((now_state, action, reward, 0))\n",
    "            record_write(file, action[0], action[1], 0)\n",
    "            now_state = next_state\n",
    "            state_vec = init_state_vec()\n",
    "            state_vec[state_index[now_state]] = 1\n",
    "\n",
    "            # 学習\n",
    "            # experiment_replayによるバッチ学習\n",
    "            perm = np.random.permutation(len(records))[:BATCH_SIZE]\n",
    "            x_batch_state   = state_vecs[perm[0:BATCH_SIZE]]\n",
    "            x_batch_action  = actions[perm[0:BATCH_SIZE]]\n",
    "            x_batch_rewords = rewords[perm[0:BATCH_SIZE]]\n",
    "            x_batch_now_state = []\n",
    "            x_batch_next_state = []\n",
    "            for index in perm:\n",
    "                x_batch_now_state.append(now_states[index])\n",
    "                x_batch_next_state.append(next_states[index])\n",
    "            x_batch_next_state_vec = next_state_vecs[perm[0:BATCH_SIZE]]\n",
    "            y_batch_target = []\n",
    "            # ニューラルの重み更新\n",
    "            for index in range(BATCH_SIZE):\n",
    "                y_batch_target.append(deep_learn(q_network, target_network, x_batch_state[index], x_batch_now_state[index], x_batch_next_state[index], 1))\n",
    "            y_batch_target = np.array(y_batch_target, dtype=np.float32)\n",
    "            q_network.init_grads()\n",
    "            loss = q_network.forward(1, x_batch_state, y_batch_target)\n",
    "            q_network.backpropagation(loss)\n",
    "              \n",
    "        # レコードの更新\n",
    "        if len(next_records) + len(best_records) > 5000:\n",
    "            if len(best_records) > 5000:\n",
    "                best_records = []\n",
    "            best_records.extend(next_records)\n",
    "            state_vecs, actions, rewords ,next_state_vecs, terminals, next_states, now_states = translate(next_records)\n",
    "            records = best_records\n",
    "            next_records = []\n",
    "        \n",
    "        # target networkの更新\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch)\n",
    "            target_network = update_target_network(q_network)\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            q_network.save_weight()\n",
    "            \n",
    "    file.close()\n",
    "    action_check_file.close()\n",
    "    return result_list\n",
    "\n",
    "def init_state_vec():\n",
    "    return np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "\n",
    "def get_state_vec(state_vec):\n",
    "    return np.array([state_vec], dtype = np.float32)\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    return (state[0]+action[0], state[1]+action[1])\n",
    "\n",
    "def deep_learn(q_network, target_network, state_vec, now_state, next_state, flg):\n",
    "    y_targets = []\n",
    "    state_index = init_index()\n",
    "    for action in ACTION:\n",
    "        next_state_vec = init_state_vec()\n",
    "        next_state = get_next_state(now_state, action)\n",
    "        # 次の状態が迷路外\n",
    "        # 報酬(罰則)のみ\n",
    "        if next_state[0] < 0 or next_state[0] > len(MAZE)-1 or next_state[1] > len(MAZE[0])-1 or next_state[1] < 0 or MAZE[next_state[0]][next_state[1]] == -1:\n",
    "            y_target = -2\n",
    "        else:\n",
    "            next_state_vec[state_index[(next_state[0], next_state[1])]] = 1\n",
    "            next_actions = target_network.forward(0, np.array([next_state_vec], dtype=np.float32))\n",
    "            max_q = np.max(next_actions.data[0], axis = 0)\n",
    "            y_target = MAZE[next_state[0]][next_state[1]] + 0.1 * max_q\n",
    "        y_targets.append(y_target)\n",
    "    y_targets = np.array(y_targets, dtype=np.float32)\n",
    "    if flg: return y_targets\n",
    "    y_target = np.array([y_targets], dtype=np.float32)\n",
    "    q_network.init_grads()\n",
    "    loss = q_network.forward(1, state_vec, y_target)\n",
    "    q_network.backpropagation(loss)\n",
    "\n",
    "def update_target_network(q_network):\n",
    "    return copy.deepcopy(q_network)\n",
    "\n",
    "def state_check(state):\n",
    "    if (state[0] < 0) or (state[1] < 0) or (len(MAZE)-1) < state[0] or (len(MAZE[0])-1 < state[1]) :\n",
    "        RESULT.append(0)\n",
    "        return 0\n",
    "    return 1 \n",
    "\n",
    "def policy_egreedy(state, neural, EPSIL):\n",
    "    qvalue = neural.forward(0, state).data[0]\n",
    "    return (ACTION[random.choice([i for i, x in enumerate(qvalue) if x == max(qvalue)])] if EPSIL < random.random() else random.choice(ACTION)), max(qvalue), qvalue           \n",
    "\n",
    "def policy_greedy_tri(state, neural, EPSIL):\n",
    "    import scipy.spatial.distance\n",
    "    qvalue_list = []\n",
    "    tmp = []\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_vec = np.array(neural.forward(0, state).data[0])\n",
    "    for qvalue in qvalue_list:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(np.array(qvalue), qvalue_vec)\n",
    "        tmp.append(sim)\n",
    "    if tmp[0] < tmp[1]:\n",
    "        return ACTION[list(qvalue_list[1]).index(max(qvalue_list[1]))], max(qvalue_list[1]), qvalue_list[1]\n",
    "    else:\n",
    "        return ACTION[list(qvalue_list[0]).index(max(qvalue_list[0]))], max(qvalue_list[0]), qvalue_list[0]\n",
    "\n",
    "def translate(records):\n",
    "    now_states = []\n",
    "    state_vecs  = []\n",
    "    actions = []\n",
    "    rewords = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    next_state_vecs = []\n",
    "    state_index = init_index()\n",
    "    state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "    next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "    for record in records:\n",
    "        now_states.append(record[0])\n",
    "        next_state = (record[0][0]+record[1][0], record[0][1]+record[1][1])\n",
    "        next_states.append(next_state)\n",
    "        state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "        state_vec[state_index[record[0]]] = 1\n",
    "        state_vecs.append(state_vec)\n",
    "        actions.append(record[1])\n",
    "        rewords.append(record[2])\n",
    "        terminals.append(record[3])\n",
    "        if record[3] == 1:\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "        else:\n",
    "            next_state_vec = np.array([0 for i in range(NUM_IN)], dtype=np.float32)\n",
    "            next_state_vec[state_index[next_state]] = 1\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "    return np.array(state_vecs, dtype=np.float32), np.array(actions), np.array(rewords, dtype=np.float32), np.array(next_state_vecs, dtype=np.float32), np.array(terminals, dtype=np.float32), next_states, now_states\n",
    "\n",
    "def experience_replay():\n",
    "    records = []\n",
    "    with open(\"./record.csv\") as f:\n",
    "        for line in f:\n",
    "            line   = line[:-1].split(\",\")\n",
    "            state  = (int(line[0]), int(line[1]))\n",
    "            action = (int(line[2]), int(line[3]))\n",
    "            reword = int(line[4])\n",
    "            terminal = int(line[5])\n",
    "            record = (state, action, reword, terminal)\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def record_write(file, state_y, state_x, terminal):\n",
    "    file.write(str(state_y) + \",\" + str(state_x) + \",\" +str(terminal) + \"\\n\")\n",
    "\n",
    "def init_index():\n",
    "    qtable_index = {}\n",
    "    num = 0\n",
    "    for y in range(len(MAZE)+1):\n",
    "        for x in range(len(MAZE[0])+1):\n",
    "            qtable_index[(y, x)] = num\n",
    "            num += 1\n",
    "    return qtable_index\n",
    "\n",
    "def decay_EPSIL(epoch, EPSIL):\n",
    "    if epoch > (EPOCH/3)*2:\n",
    "        return EPSIL/(epoch)*(EPOCH/10)\n",
    "    return EPSIL\n",
    "\n",
    "def main():\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    q_network.load_weight()\n",
    "    target_network = update_target_network(q_network)\n",
    "    return deep_q_learn(q_network, target_network, experience_replay())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(1, 3)\n",
      "(3, 4)\n",
      "(3, 4)\n",
      "(7, 0)\n",
      "5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'action_check_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4236e4443c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgragh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2224263d8dbc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mtarget_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeep_q_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2224263d8dbc>\u001b[0m in \u001b[0;36mdeep_q_learn\u001b[0;34m(q_network, target_network, records)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0maction_check_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action_check_file' is not defined"
     ]
    }
   ],
   "source": [
    "_list = main()\n",
    "gragh(_list, len(_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hid1, num_hid2, num_hid3, num_out):\n",
    "        self.model = Chain(hid_layer1 = L.Linear(num_in, num_hid1),\n",
    "                           hid_layer2 = L.Linear(num_hid1, num_hid2),\n",
    "                           hid_layer3 = L.Linear(num_hid2, num_hid3),\n",
    "                           out_layer  = L.Linear(num_hid3, num_out))\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "    \n",
    "    def forward(self, flg, x, t = None):\n",
    "        _x = Variable(x)\n",
    "        if flg == 1: _t = Variable(t)\n",
    "        h1  = F.dropout(F.relu(self.model.hid_layer1(_x)))\n",
    "        h2  = F.dropout(F.relu(self.model.hid_layer2(h1)))\n",
    "        h3  = F.dropout(F.relu(self.model.hid_layer3(h2)))\n",
    "        u3  = self.model.out_layer(h3)\n",
    "        # return F.softmax_cross_entropy(u2, _t) if flg else F.softmax(u2)\n",
    "        # return F.mean_squared_error(self.policy_greedy(u3), _t) if flg else u3\n",
    "        return F.mean_squared_error(u3, _t) if flg else u3\n",
    "    \n",
    "    def backpropagation(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "    \n",
    "    def init_grads(self):\n",
    "        self.optimizer.zero_grads()\n",
    "        \n",
    "    def save_weight(self):\n",
    "        serializers.save_npz(\"my.model\", self.model)\n",
    "        \n",
    "    def load_weight(self):\n",
    "        serializers.load_npz(\"my.model\", self.model)\n",
    "        \n",
    "    def policy_greedy(self, actions):\n",
    "        return np.max(actions.data, axis = 1)\n",
    "    \n",
    "class Gragh:\n",
    "    def gragh(result, epoch):    \n",
    "        x = np.arange(0, epoch, 1)\n",
    "        left = np.array(x)\n",
    "        count = 0\n",
    "        count_1 = 0\n",
    "        parcent = []\n",
    "        for i in result:\n",
    "            count += 1\n",
    "            if i == 1:\n",
    "                count_1 += 1\n",
    "            parcent.append(count_1/count)\n",
    "        height = np.array(parcent)\n",
    "        plt.plot(left, height)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gragh(result, epoch):    \n",
    "    x = np.arange(0, epoch, 1)\n",
    "    left = np.array(x)\n",
    "    count = 0\n",
    "    count_1 = 0\n",
    "    parcent = []\n",
    "    for i in result:\n",
    "        count += 1\n",
    "        if i == 1:\n",
    "            count_1 += 1\n",
    "        parcent.append(count_1/count)\n",
    "    height = np.array(parcent)\n",
    "    plt.plot(left, height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
