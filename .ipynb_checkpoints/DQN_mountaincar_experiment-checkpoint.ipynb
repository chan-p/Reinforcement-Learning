{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import gym\n",
    "\n",
    "NUM_IN   = 2\n",
    "NUM_HID1 = 1000\n",
    "NUM_HID2 = 500\n",
    "NUM_HID3 = 250\n",
    "NUM_OUT  = 3\n",
    "BATCH_SIZE = 32\n",
    "ALPH   = 0.1\n",
    "ACTION = [-2, 0, 2]\n",
    "EPOCH = 100\n",
    "GAMMA = 0.99\n",
    "DQNMODEL_PATH = \"./DQN_model/mounaincar.model\"\n",
    "\n",
    "def deep_q_learn(env, q_network, target_network, experince):\n",
    "    # プロッセサーの選択\n",
    "    xp = switch_ProccerUnit(\"CPU\")\n",
    "    epsil = 0.2\n",
    "    for episode in range(1, EPOCH+1):\n",
    "        # 初期状態の定義\n",
    "        now_state = env.reset()\n",
    "        pos, spe = get_state(now_state)\n",
    "        state_vec = get_state_vec(pos, spe, xp, 2)\n",
    "        epsil = reduce_epsil(episode, epsil)\n",
    "        for step in range(200):\n",
    "            action, max_q = policy_egreedy_tri(env, state_vec, q_network, epsil, xp)\n",
    "            next_state, reward, terminal, info = agent_action(env, action)\n",
    "            deep_learn(q_network, target_network, state_vec, env, xp)\n",
    "            if reward == 1:\n",
    "                break\n",
    "            # 状態更新\n",
    "            pos, spe = get_state(next_state)\n",
    "            state_vec = get_state_vec(pos, spe, xp, 2)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            q_network.save_weight(DQNMODEL_PATH)\n",
    "        \n",
    "def get_state(state):\n",
    "    return state[0], state[1]\n",
    "                 \n",
    "\n",
    "def get_state_vec(pos, spe, xp, flg):\n",
    "    vec = xp.array([pos, spe], dtype=xp.float32)\n",
    "    if flg == 1: return vec\n",
    "    return xp.array([vec], dtype=xp.float32) \n",
    "        \n",
    "    \n",
    "    \n",
    "def deep_learn(q_network, target_network, state_vec, env, xp, flg=None):\n",
    "    y_targets = []\n",
    "    for action in range(len(ACTION)):\n",
    "        # 環境をコピー\n",
    "        env_try = copy.deepcopy(env)\n",
    "        # 行動による次状態を観測\n",
    "        next_state, reward, terminal, info = agent_action(env_try, action)\n",
    "        next_pos, next_spe = get_state(next_state)\n",
    "        # 状態を観測した次状態として次行動を選択\n",
    "        next_action, max_q = policy_egreedy_tri(env_try, get_state_vec(next_pos, next_spe, xp, 2), target_network, 0, xp)\n",
    "        # 次行動よる次々状態を観測\n",
    "        next_next_state, reward, terminal, info = agent_action(env_try, next_action)\n",
    "        # 教師データ生成\n",
    "        y_target = reward + GAMMA * max_q    \n",
    "        y_targets.append(y_target)\n",
    "    y_targets = xp.array(y_targets, dtype=np.float32)\n",
    "    if flg: return y_targets\n",
    "    y_target = xp.array([y_targets], dtype=np.float32)\n",
    "    q_network.init_grads()\n",
    "    loss = q_network.forward(1, state_vec, y_target)\n",
    "    q_network.backpropagation(loss)\n",
    "\n",
    "def policy_egreedy_tri(env, state, neural, epsil, xp):\n",
    "    import scipy.spatial.distance\n",
    "    qvalue_list = []\n",
    "    tmp = []\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_vec = np.array(neural.forward(0, state).data[0])\n",
    "    for qvalue in qvalue_list:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(xp.array(qvalue), qvalue_vec)\n",
    "        tmp.append(sim)\n",
    "    if tmp[0] < tmp[1]:\n",
    "        return list(qvalue_list[1]).index(max(qvalue_list[1])), max(qvalue_list[1])\n",
    "    else:\n",
    "        return list(qvalue_list[0]).index(max(qvalue_list[0])), max(qvalue_list[0])\n",
    "        \n",
    "def agent_action(env, action):\n",
    "    return env.step(action)\n",
    "\n",
    "def update_target_network(q_network):\n",
    "    return copy.deepcopy(q_network)\n",
    "\n",
    "def reduce_epsil(epoch, epsil):\n",
    "    return epsil\n",
    "                 \n",
    "def switch_ProccerUnit(pu):\n",
    "    return cuda.cupy if pu == \"GPU\" else np\n",
    "                 \n",
    "def main():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    # q_network.load_weight()\n",
    "    target_network = update_target_network(q_network)\n",
    "    deep_q_learn(env, q_network, target_network, [0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 500\n",
    "def start(env, q_network, target_network, experince):\n",
    "    # プロッセサーの選択\n",
    "    xp = switch_ProccerUnit(\"CPU\")\n",
    "    epsil = 0.2\n",
    "    for episode in range(1, EPOCH+1):\n",
    "        # 初期状態の定義\n",
    "        now_state = env.reset()\n",
    "        pos, spe = get_state(now_state)\n",
    "        state_vec = get_state_vec(pos, spe, xp, 2)\n",
    "        epsil = reduce_epsil(episode, epsil)\n",
    "        action, max_q = policy_egreedy_tri(env, state_vec, q_network, epsil, xp)\n",
    "        next_state, reward, terminal, info = agent_action(env, action)\n",
    "        if reward == 1:\n",
    "            break\n",
    "        # 状態更新\n",
    "        pos, spe = get_state(next_state)\n",
    "        state_vec = get_state_vec(pos, spe, xp, 2)\n",
    "        env.render()\n",
    "        \n",
    "def experiment():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    q_network.load_weight(DQNMODEL_PATH)\n",
    "    target_network = update_target_network(q_network)\n",
    "    start(env, q_network, target_network, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-12 11:18:00,365] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hid1, num_hid2, num_hid3, num_out):\n",
    "        self.model = Chain(hid_layer1 = L.Linear(num_in, num_hid1),\n",
    "                           hid_layer2 = L.Linear(num_hid1, num_hid2),\n",
    "                           hid_layer3 = L.Linear(num_hid2, num_hid3),\n",
    "                           out_layer  = L.Linear(num_hid3, num_out))\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "    \n",
    "    def forward(self, flg, x, t = None):\n",
    "        _x = Variable(x)\n",
    "        if flg == 1: _t = Variable(t)\n",
    "        h1  = F.dropout(F.relu(self.model.hid_layer1(_x)))\n",
    "        h2  = F.dropout(F.relu(self.model.hid_layer2(h1)))\n",
    "        h3  = F.dropout(F.relu(self.model.hid_layer3(h2)))\n",
    "        u3  = self.model.out_layer(h3)\n",
    "        # return F.softmax_cross_entropy(u2, _t) if flg else F.softmax(u2)\n",
    "        # return F.mean_squared_error(self.policy_greedy(u3), _t) if flg else u3\n",
    "        return F.mean_squared_error(u3, _t) if flg else u3\n",
    "    \n",
    "    def backpropagation(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "    \n",
    "    def init_grads(self):\n",
    "        self.optimizer.zero_grads()\n",
    "        \n",
    "    def save_weight(self, model):\n",
    "        serializers.save_npz(model, self.model)\n",
    "        \n",
    "    def load_weight(self, model):\n",
    "        serializers.load_npz(model, self.model)\n",
    "        \n",
    "    def policy_greedy(self, actions):\n",
    "        return np.max(actions.data, axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
