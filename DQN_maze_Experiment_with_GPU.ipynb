{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     140,
     143,
     146,
     149,
     172,
     175,
     181,
     185,
     189,
     207,
     237,
     250,
     254,
     263,
     268
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers, cuda\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0,-1, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1],\n",
    "        [-1,-1, 0,-1,-1,-1,-1,-1,-9,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0, 0, 1,-1],\n",
    "        [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]]\n",
    "\"\"\"\n",
    "MAZE = [[-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [-1,-1,-1,-1,-1, 0,-1,-1, 0,-1,-1,-1],\n",
    "        [-1,-1, 0, 0, 0, 0, 0,-1, 0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [-1, 0, 0, 0, 0, 0, 0, 0 ,0,-1,-1,-1],\n",
    "        [-1, 0,-1,-1,-1,-1,-1,-1, 0,-1,-1,-1],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1, 0,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0, 0, 0, 0,-1,-1,-1],\n",
    "        [ 0,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1],\n",
    "        [ 0,-1, 0, 0, 0, 0,-1, 0, 0, 0, 1,-1],\n",
    "        [ 0, 0, 0,-1,-1, 0, 0, 0,-1,-1,-1,-1]]\n",
    "START  = (1, 1)\n",
    "ACTION = [(-1, 0), (1, 0), (0, -1), (0, 1)] # [上, 下, 左, 右]\n",
    "EPOCH  = 50\n",
    "RESULT = []\n",
    "NUM_IN   = (len(MAZE)+1) * (len(MAZE[0])+1)\n",
    "NUM_HID1 = 1000\n",
    "NUM_HID2 = 500\n",
    "NUM_HID3 = 250\n",
    "NUM_OUT  = 4\n",
    "BATCH_SIZE = 50\n",
    "ALPH   = 0.1\n",
    "GAMMA  = np.array([0.99 for i in range(BATCH_SIZE)], np.float32)\n",
    "\n",
    "def deep_q_learn(q_network, target_network, records):\n",
    "    result_list  = []\n",
    "    next_records = []\n",
    "    best_records = []\n",
    "    state_index  = init_index()\n",
    "    EPSIL  = 0.01\n",
    "    # GPUを使う用の高速演算ライブラリ\n",
    "    cuda.get_device(0).use()\n",
    "    xp = cuda.cupy\n",
    "    \n",
    "    state_vecs, actions, rewords ,next_state_vecs, terminals, next_states, now_states= translate(records, xp)\n",
    "    for epoch in range(EPOCH):\n",
    "        EPSIL = decay_EPSIL(epoch, EPSIL)\n",
    "        state_vec = init_state_vec(xp)\n",
    "        now_state = START\n",
    "        state_vec[state_index[START]] = 1\n",
    "        # record_write(file, 0, 0, 3)\n",
    "        while True:\n",
    "            # 状態sをDNN用にベクトル化\n",
    "            state_vec = get_state_vec(state_vec, xp)\n",
    "            # 状態sをDNNの入力として状態sにおける各行動aの行動価値を算出：Q(s,a)\n",
    "            # 方策：e-greedy\n",
    "            action, q_max, action_list = policy_greedy_tri(state_vec, q_network, EPSIL, xp)\n",
    "            # 次の状態s'を決定\n",
    "            next_state = get_next_state(now_state, action)\n",
    "            # 次の状態が迷路外ならエピソード終了\n",
    "            if state_check(next_state) == 0 or MAZE[next_state[0]][next_state[1]] == -1:\n",
    "                print(now_state)\n",
    "                # 罰則による学習\n",
    "                deep_learn(q_network, target_network, state_vec, now_state, next_state, 0, xp)\n",
    "                # record_write(file, action[0], action[1], 1)\n",
    "                next_records.append((now_state, action, -1, 1))\n",
    "                result_list.append(0)\n",
    "                break\n",
    "            # 即時報酬\n",
    "            reward = MAZE[next_state[0]][next_state[1]]\n",
    "            if reward == 1:\n",
    "                print(epoch)\n",
    "                # 成功報酬による学習\n",
    "                deep_learn(q_network, target_network, state_vec, now_state, next_state, 0, xp)\n",
    "                # record_write(file, 0, 0, 2)\n",
    "                # 成功体験を優先してExperiment_Replayレコードに追加\n",
    "                best_records.append((now_state, action, 1, 1))\n",
    "                result_list.append(1)\n",
    "                break\n",
    "            # 実行動による学習\n",
    "            deep_learn(q_network, target_network, state_vec, now_state, next_state, 1, xp)\n",
    "            # Experiment_Replayレコードに追加\n",
    "            next_records.append((now_state, action, reward, 0))\n",
    "            # record_write(file, action[0], action[1], 0)\n",
    "            now_state = next_state\n",
    "            state_vec = init_state_vec(xp)\n",
    "            state_vec[state_index[now_state]] = 1\n",
    " \n",
    "        # レコードの更新\n",
    "        if len(next_records) + len(best_records) > 3000:\n",
    "            if len(best_records) > 200:\n",
    "                best_records = []\n",
    "            next_records.extend(best_records)\n",
    "            state_vecs, actions, rewords ,next_state_vecs, terminals, next_states, now_states = translate(next_records, xp)\n",
    "            records = next_records\n",
    "            next_records = []\n",
    "        \n",
    "        # target networkの更新\n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch)\n",
    "            target_network = update_target_network(q_network)\n",
    "    # file.close()\n",
    "    return result_list\n",
    "\n",
    "def init_state_vec(xp):\n",
    "    return xp.array([0 for i in range(NUM_IN)], dtype=xp.float32)\n",
    "\n",
    "def get_state_vec(state_vec, xp):\n",
    "    return xp.array([state_vec], dtype = xp.float32)\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    return (state[0]+action[0], state[1]+action[1])\n",
    "\n",
    "def deep_learn(q_network, target_network, state_vec, now_state, next_state, flg, xp):\n",
    "    y_targets = []\n",
    "    state_index = init_index()\n",
    "    for action in ACTION:\n",
    "        next_state_vec = init_state_vec(xp)\n",
    "        next_state = get_next_state(now_state, action)\n",
    "        # 次の状態が迷路外\n",
    "        # 報酬(罰則)のみ\n",
    "        if next_state[0] < 0 or next_state[0] > len(MAZE)-1 or next_state[1] > len(MAZE[0])-1 or next_state[1] < 0 or MAZE[next_state[0]][next_state[1]] == -1:\n",
    "            y_target = -2\n",
    "        else:\n",
    "            next_state_vec[state_index[(next_state[0], next_state[1])]] = 1\n",
    "            next_actions = target_network.forward(0, xp.array([next_state_vec], dtype=xp.float32))\n",
    "            max_q = xp.max(next_actions.data[0], axis = 0)\n",
    "            y_target = MAZE[next_state[0]][next_state[1]] + 0.1 * max_q\n",
    "        y_targets.append(y_target)\n",
    "    y_targets = xp.array(y_targets, dtype=xp.float32)\n",
    "    if flg: return y_targets\n",
    "    y_target = xp.array([y_targets], dtype=xp.float32)\n",
    "    q_network.init_grads()\n",
    "    loss = q_network.forward(1, state_vec, y_target)\n",
    "    q_network.backpropagation(loss)\n",
    "\n",
    "def update_target_network(q_network):\n",
    "    return copy.deepcopy(q_network)\n",
    "\n",
    "def state_check(state):\n",
    "    if (state[0] < 0) or (state[1] < 0) or (len(MAZE)-1) < state[0] or (len(MAZE[0])-1 < state[1]) :\n",
    "        RESULT.append(0)\n",
    "        return 0\n",
    "    return 1 \n",
    "\n",
    "def policy_egreedy(state, neural, EPSIL):\n",
    "    qvalue = neural.forward(0, state).data[0]\n",
    "    return (ACTION[random.choice([i for i, x in enumerate(qvalue) if x == max(qvalue)])] if EPSIL < random.random() else random.choice(ACTION)), max(qvalue), qvalue           \n",
    "\n",
    "def policy_greedy(state, neural, EPSIL):\n",
    "    qvalue = neural.forward(0, state).data[0]\n",
    "    return ACTION[list(qvalue).index(max(qvalue))], max(qvalue), qvalue   \n",
    "\n",
    "def policy_greedy_tri(state, neural, EPSIL, xp):\n",
    "    qvalue_list = []\n",
    "    tmp = []\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_list.append(neural.forward(0, state).data[0])\n",
    "    qvalue_vec = xp.array(neural.forward(0, state).data[0])\n",
    "    for qvalue in qvalue_list:\n",
    "        sim = 1 - scipy.spatial.distance.cosine(cuda.to_cpu(qvalue), cuda.to_cpu(qvalue_vec))\n",
    "        tmp.append(sim)\n",
    "    if tmp[0] < tmp[1]:\n",
    "        return ACTION[list(qvalue_list[1]).index(max(qvalue_list[1]))], max(qvalue_list[1]), qvalue_list[1]\n",
    "    else:\n",
    "        return ACTION[list(qvalue_list[0]).index(max(qvalue_list[0]))], max(qvalue_list[0]), qvalue_list[0]\n",
    "\n",
    "def translate(records, xp):\n",
    "    now_states = []\n",
    "    state_vecs  = []\n",
    "    actions = []\n",
    "    rewords = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    next_state_vecs = []\n",
    "    state_index = init_index()\n",
    "    state_vec = xp.array([0 for i in range(NUM_IN)], dtype=xp.float32)\n",
    "    next_state_vec = xp.array([0 for i in range(NUM_IN)], dtype=xp.float32)\n",
    "    for record in records:\n",
    "        now_states.append(record[0])\n",
    "        next_state = (record[0][0]+record[1][0], record[0][1]+record[1][1])\n",
    "        next_states.append(next_state)\n",
    "        state_vec = xp.array([0 for i in range(NUM_IN)], dtype=xp.float32)\n",
    "        state_vec[state_index[record[0]]] = 1\n",
    "        state_vecs.append(state_vec)\n",
    "        actions.append(record[1])\n",
    "        rewords.append(record[2])\n",
    "        terminals.append(record[3])\n",
    "        if record[3] == 1:\n",
    "            next_state_vec = xp.array([0 for i in range(NUM_IN)], dtype=xp.float32)\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "        else:\n",
    "            next_state_vec = xp.array([0 for i in range(NUM_IN)], dtype=xp.float32)\n",
    "            next_state_vec[state_index[next_state]] = 1\n",
    "            next_state_vecs.append(next_state_vec)\n",
    "    return xp.array(state_vecs, dtype=xp.float32), xp.array(actions), xp.array(rewords, dtype=xp.float32), xp.array(next_state_vecs, dtype=xp.float32), xp.array(terminals, dtype=xp.float32), next_states, now_states\n",
    "\n",
    "def experience_replay():\n",
    "    records = []\n",
    "    with open(\"./record.csv\") as f:\n",
    "        for line in f:\n",
    "            line   = line[:-1].split(\",\")\n",
    "            state  = (int(line[0]), int(line[1]))\n",
    "            action = (int(line[2]), int(line[3]))\n",
    "            reword = int(line[4])\n",
    "            terminal = int(line[5])\n",
    "            record = (state, action, reword, terminal)\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def record_write(file, state_y, state_x, terminal):\n",
    "    file.write(str(state_y) + \",\" + str(state_x) + \",\" +str(terminal) + \"\\n\")    \n",
    "\n",
    "def init_index():\n",
    "    qtable_index = {}\n",
    "    num = 0\n",
    "    for y in range(len(MAZE)+1):\n",
    "        for x in range(len(MAZE[0])+1):\n",
    "            qtable_index[(y, x)] = num\n",
    "            num += 1\n",
    "    return qtable_index\n",
    "\n",
    "def decay_EPSIL(epoch, EPSIL):\n",
    "    if epoch > (EPOCH/3)*2:\n",
    "        return EPSIL/(epoch)*(EPOCH/10)\n",
    "    return EPSIL\n",
    "\n",
    "def main():\n",
    "    q_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    target_network = NeuralNetwork(NUM_IN, NUM_HID1, NUM_HID2, NUM_HID3, NUM_OUT)\n",
    "    q_network.load_weight()\n",
    "    q_network.use_GPU()\n",
    "    target_network.use_GPU()\n",
    "    target_network = update_target_network(q_network)\n",
    "    return deep_q_learn(q_network, target_network, experience_replay())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "(9, 8)\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "(9, 8)\n",
      "(9, 8)\n",
      "(9, 8)\n",
      "(7, 8)\n",
      "35\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1a6619229996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0m_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgragh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"[sec]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9a3602a867b6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_GPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mtarget_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeep_q_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9a3602a867b6>\u001b[0m in \u001b[0;36mdeep_q_learn\u001b[0;34m(q_network, target_network, records)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mbest_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mnext_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mstate_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewords\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnext_state_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mnext_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9a3602a867b6>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(records, xp)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mnext_state_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mnext_state_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.local/lib/python3.6/site-packages/cupy/creation/from_data.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(obj, dtype, copy, ndmin)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# TODO(beam2d): Support order and subok options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.array (cupy/core/core.cpp:58029)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.array (cupy/core/core.cpp:57640)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "_list = main()\n",
    "gragh(_list, len(_list))\n",
    "print(str(time.time() - start) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gragh(result, epoch):    \n",
    "    x = np.arange(0, epoch, 1)\n",
    "    left = np.array(x)\n",
    "    count = 0\n",
    "    count_1 = 0\n",
    "    parcent = []\n",
    "    for i in result:\n",
    "        count += 1\n",
    "        if i == 1:\n",
    "            count_1 += 1\n",
    "        parcent.append(count_1/count)\n",
    "    height = np.array(parcent)\n",
    "    plt.plot(left, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import chainer\n",
    "from chainer import Function, Variable, optimizers, serializers, cuda\n",
    "from chainer import Link, Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_in, num_hid1, num_hid2, num_hid3, num_out):\n",
    "        self.model = Chain(hid_layer1 = L.Linear(num_in, num_hid1),\n",
    "                           hid_layer2 = L.Linear(num_hid1, num_hid2),\n",
    "                           hid_layer3 = L.Linear(num_hid2, num_hid3),\n",
    "                           out_layer  = L.Linear(num_hid3, num_out))\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.optimizer.setup(self.model)\n",
    "    \n",
    "    def forward(self, flg, x, t = None):\n",
    "        _x = Variable(x)\n",
    "        if flg == 1: _t = Variable(t)\n",
    "        h1  = F.dropout(F.relu(self.model.hid_layer1(_x)))\n",
    "        h2  = F.dropout(F.relu(self.model.hid_layer2(h1)))\n",
    "        h3  = F.dropout(F.relu(self.model.hid_layer3(h2)))\n",
    "        u3  = self.model.out_layer(h3)\n",
    "        # return F.softmax_cross_entropy(u2, _t) if flg else F.softmax(u2)\n",
    "        # return F.mean_squared_error(self.policy_greedy(u3), _t) if flg else u3\n",
    "        return F.mean_squared_error(u3, _t) if flg else u3\n",
    "\n",
    "    def backpropagation(self, loss):\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "\n",
    "    def init_grads(self):\n",
    "        self.optimizer.zero_grads()\n",
    "\n",
    "    def save_weight(self):\n",
    "        serializers.save_npz(\"my.model\", self.model)\n",
    "\n",
    "    def load_weight(self):\n",
    "        serializers.load_npz(\"my.model\", self.model)\n",
    "\n",
    "    def policy_greedy(self, actions):\n",
    "        return np.max(actions.data, axis = 1)\n",
    "\n",
    "    def use_GPU(self):\n",
    "        gpu_device = 0\n",
    "        self.model.to_gpu(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting copy\n",
      "\u001b[31m  Could not find a version that satisfies the requirement copy (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for copy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
